{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 252A Computer Vision I Fall 2017\n",
    "## Assignment 3\n",
    "\n",
    "---\n",
    "This assignment contains theoretical and programming exercises. If you plan to submit hand written answers for theoretical exercises, please be sure your writing is readable and merge those in order with the final pdf you create out of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Epipolar Geometry [3 pts]\n",
    "Consider two cameras whose image planes are the z=1 plane, and whose focal points are at (-25, 0, 0) and (25, 0, 0). We‟ll call a point in the first camera (x, y), and a point in the second camera (u, v). Points in each camera are relative to the camera center. So, for example if (x, y) = (0, 0), this is really the point (-25, 0, 1) in world coordinates, while if (u, v) = (0, 0) this is the point (25, 0, 1).\n",
    "<img src = \"fig/fig1.png\",  width=\"500\">\n",
    "a) Suppose the points (x, y) = (8, 8) is matched with disparity of 7 to the point (u, v) = (1, 8). What is the 3D location of this point?\n",
    "\n",
    "b) Consider points that lie on the line x + z = 0, y = 0. Use the same stereo set up as before. Write an analytic expression giving the disparity of a point on this line after it projects onto the two images, as a function of its position in the right image. So your expression should only involve the variables u and d (for disparity). Your expression only needs to be valid for points on the line that are in front of the cameras, i.e. with z > 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Epipolar Rectification [4 pts]\n",
    "In stereo vision, image rectification is a common preprocessing step to simplify the problem of finding\n",
    "matching points between images. The goal is to warp image views such that the epipolar lines are\n",
    "horizontal scan lines of the input images. Suppose that we have captured two images $I_A$ and $I_B$ from\n",
    "identical calibrated cameras separated by a rigid transformation\n",
    "\n",
    "$_{A}^{B}\\textrm{T}= \\begin{bmatrix}\n",
    "R & t \\\\ \n",
    "0^T & 1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Without loss of generality assume that camera A's optical center is positioned at the origin and that its optical axis is in the direction of the z-axis.\n",
    "\n",
    "From the lecture, a rectifying transform for each image should map the epipole to the point infinitely far away in the horizontal direction $ H_{A}e_{A} = H_{B}e_{B} = [1, 0, 0]^T$. Consider the following special cases:\n",
    "\n",
    "a) Pure horizontal translation $t = [tx, 0, 0]^T$, R = I\n",
    "\n",
    "b) Pure translation orthogonal to the optical axis $t = [tx, ty, 0]^T$, R = I\n",
    "\n",
    "c) Pure translation along the optical axis $t = [0, 0, tz]^T$, R = I\n",
    "\n",
    "d) Pure rotation $t = [0, 0, 0]^T$, R is an arbitrary rotation matrix\n",
    "\n",
    "For each of these cases, determine whether or not epipolar rectification is possible. Include the following information for each case\n",
    "* The epipoles $e_A$ and $e_B$\n",
    "* The equation of the epipolar line $l_B$ in $I_B$ corresponding to the point $[x_A, y_A, 1]^T$ in $I_A$ (if one exists)\n",
    "* A plausible solution to the rectifying transforms $H_A$ and $H_B$ (if one exists) that attempts to minimize distortion (is as close as possible to a 2D rigid transformation). Note that the above 4 cases are special cases; a simple solution should become apparent by looking at the epipolar lines.\n",
    "\n",
    "One or more of the above rigid transformations may be a degenerate case where rectification is not\n",
    "possible or epipolar geometry does not apply. If so, explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: NCC [3 pts]\n",
    "Show that maximizing the NCC $\\sum_{i,j}\\tilde{W_1} (i,j)\\cdot \\tilde{W_2} (i,j)$ is equivalent to minimizing the NSSD $\\sum_{i,j}\\left | \\tilde{W_1} (i,j) - \\tilde{W_2} (i,j) \\right |^2$.\n",
    "\n",
    "Hint: Express $c_{NCC}$ and $c_{NSSD}$ in terms of the vectors $\\tilde{w_1} =  \\tilde{W_1} (:)$\n",
    "and $\\tilde{w_2} =  \\tilde{W_2} (:)$. Thus, $c_{NCC} = \\tilde{w_1}^T\\tilde{w_2}$ and $c_{NSSD} = (\\tilde{w_1} - \\tilde{w_2})^T(\\tilde{w_1} - \\tilde{w_2})$,\n",
    "\n",
    "$\\tilde{W} = \\frac{W - \\overline{W}}{\\sqrt{\\sum_{k,l}(W(k,l) - \\overline{W})^2}}$ is a mean-shifted and normalized version of the window. N refers to the number of pixels in each window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Sparse Stereo Matching [18 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this problem we will play around with sparse stereo matching methods. You will work on two image pairs, a warrior figure and a figure from the Matrix movies. These files both contain two images, two camera matrices, and set sets of corresponding points (extracted by manually clicking the images). For illustration, I have run my code on a third image pair (dino1.png, dino2.png). This data is also provided for you to debug your code, but you should only report results on warrior and matrix. In other words, where I include one (or a pair) of images in the assignment below, you will provide the same thing but for BOTH matrix and warrior. Note that the matrix image pair is harder, in the sense that the matching algorithms we are implementing will not work quite as well. You should expect good results, however, on warrior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corner Detection [3 pts]\n",
    "The first thing we need to do is to build a corner detector. This should be done according to http://cseweb.ucsd.edu/classes/fa17/cse252A-a/lec11.pdf. Your should fill in the function <code>corner_detect</code> below, and take as <code>input corner_detect(image, nCorners, smoothSTD, windowSize)</code> where smoothSTD is the standard deviation of the smoothing kernel and windowSize is the window size for corner detector and non maximum suppression. In the lecture the corner detector was implemented using a hard threshold. Do not do that but instead return the nCorners strongest corners after non-maximum suppression. This way you can control exactly how many corners are returned. Run your code on all four images (with nCorners = 20) and show outputs as in Figure 2. You may find scipy.ndimage.filters.gaussian_filter easy to use for smoothing. In this problem try different parameters and then comment on results.\n",
    "1. windowSize = 3, 5, 9, 17\n",
    "2. smoothSTD = 0.5, 1, 2, 4\n",
    "<img src = \"fig/dinoCorner1.png\", alt=\"dino Corner 1\" width=\"400\"/> <img src = \"fig/dinoCorner2.png\", alt=\"dino Corner 2\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.misc import imread\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage.filters import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    \"\"\" Convert rgb image to grayscale.\n",
    "    \"\"\"\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corner_detect(image, nCorners, smoothSTD, windowSize):\n",
    "    \"\"\"Detect corners on a given image.\n",
    "\n",
    "    Args:\n",
    "        image: Given a grayscale image on which to detect corners.\n",
    "        nCorners: Total number of corners to be extracted.\n",
    "        smoothSTD: Standard deviation of the Gaussian smoothing kernel.\n",
    "        windowSize: Window size for corner detector and non maximum suppression.\n",
    "\n",
    "    Returns:\n",
    "        Detected corners (in image coordinate) in a numpy array (n*2).\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Your code here:\n",
    "    \"\"\"\n",
    "    corners = np.zeros((nCorners, 2))\n",
    "    return corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect corners on warrior and matrix sets\n",
    "# adjust your corner detection parameters here\n",
    "nCorners = 20\n",
    "smoothSTD = 2\n",
    "windowSize = 11\n",
    "\n",
    "# read images and detect corners on images\n",
    "imgs_mat = []\n",
    "crns_mat = []\n",
    "imgs_war = []\n",
    "crns_war = []\n",
    "for i in range(2):\n",
    "    img_mat = imread('p4/matrix/matrix' + str(i) + '.png')\n",
    "    imgs_mat.append(rgb2gray(img_mat))\n",
    "    # downsize your image in case corner_detect runs slow in test\n",
    "    # imgs_mat.append(rgb2gray(img_mat)[::2, ::2])\n",
    "    crns_mat.append(corner_detect(imgs_mat[i], nCorners, smoothSTD, windowSize))\n",
    "    \n",
    "    img_war = imread('p4/warrior/warrior' + str(i) + '.png')\n",
    "    imgs_war.append(rgb2gray(img_war))\n",
    "    # downsize your image in case corner_detect runs slow in test\n",
    "    # imgs_war.append(rgb2gray(img_war)[::2, ::2])\n",
    "    crns_war.append(corner_detect(imgs_war[i], nCorners, smoothSTD, windowSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_corners_result(imgs, corners):\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax1 = fig.add_subplot(221)\n",
    "    ax1.imshow(imgs[0], cmap='gray')\n",
    "    ax1.scatter(corners[0][:, 0], corners[0][:, 1], s=35, edgecolors='r', facecolors='none')\n",
    "\n",
    "    ax2 = fig.add_subplot(222)\n",
    "    ax2.imshow(imgs[1], cmap='gray')\n",
    "    ax2.scatter(corners[1][:, 0], corners[1][:, 1], s=35, edgecolors='r', facecolors='none')\n",
    "    plt.show()\n",
    "\n",
    "show_corners_result(imgs_mat, crns_mat)\n",
    "show_corners_result(imgs_war, crns_war)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSD Matching [1 pts]\n",
    "\n",
    "Write a function ssd_match that implements the SSD matching algorithm for two input windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ssd_match(img1, img2, c1, c2, R):\n",
    "    \"\"\"Compute SSD given two windows.\n",
    "\n",
    "    Args:\n",
    "        img1: Image 1.\n",
    "        img2: Image 2.\n",
    "        c1: Center (in image coordinate) of the window in image 1.\n",
    "        c2: Center (in image coordinate) of the window in image 2.\n",
    "        R: R is the radius of the patch, 2 * R + 1 is the window size\n",
    "\n",
    "    Returns:\n",
    "        SSD matching score for two input windows.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Your code here:\n",
    "    \"\"\"\n",
    "    matching_score = 0\n",
    "    return matching_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test SSD match\n",
    "img1 = np.array([[1, 2, 3, 4], [4, 5, 6, 8], [7, 8, 9, 4]])\n",
    "img2 = np.array([[1, 2, 1, 3], [6, 5, 4, 4], [9, 8, 7, 3]])\n",
    "print ssd_match(img1, img2, np.array([1, 1]), np.array([1, 1]), 1)\n",
    "# should print 20\n",
    "print ssd_match(img1, img2, np.array([2, 1]), np.array([2, 1]), 1)\n",
    "# should print 20\n",
    "print ssd_match(img1, img2, np.array([1, 1]), np.array([2, 1]), 1)\n",
    "# should print 46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Matching [3 pts]\n",
    "\n",
    "Equipped with the corner detector and the SSD matching function, we are ready to start finding correspondances. One naive strategy is to try and find the best match between the two sets of corner points. Write a script that does this, namely, for each corner in image1, find the best match from the detected corners in image2 (or, if the SSD match score is too low, then return no match for that point). You will have to figure out a good threshold (SSDth) value by experimentation. Write a function naiveCorrespondanceMatching.m and call it as below. Examine your results for 10, 20, and 30 detected corners in each image. Choose a number of detected corners to the maximize the number of correct matching pairs. naive_matching will call your SSD mathching code. <img src = \"fig/dinoMatch.png\", alt=\"dino match\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matching(img1, img2, corners1, corners2, R, SSDth):\n",
    "    \"\"\"Compute SSD given two windows.\n",
    "\n",
    "    Args:\n",
    "        img1: Image 1.\n",
    "        img2: Image 2.\n",
    "        corners1: Corners in image 1 (nx2)\n",
    "        corners2: Corners in image 2 (nx2)\n",
    "        R: SSD matching radius\n",
    "        SSDth: SSD matching score threshold\n",
    "\n",
    "    Returns:\n",
    "        SSD matching result a list of tuple (c1, c2), \n",
    "        c1 is the 1x2 corner location in image 1, \n",
    "        c2 is the 1x2 corner location in image 2. \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Your code here:\n",
    "    \"\"\"\n",
    "    matching = []\n",
    "    return matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect corners on warrior and matrix sets\n",
    "# adjust your corner detection parameters here\n",
    "nCorners = 20\n",
    "smoothSTD = 2\n",
    "windowSize = 11\n",
    "\n",
    "# read images and detect corners on images\n",
    "imgs_mat = []\n",
    "crns_mat = []\n",
    "imgs_war = []\n",
    "crns_war = []\n",
    "for i in range(2):\n",
    "    img_mat = imread('p4/matrix/matrix' + str(i) + '.png')\n",
    "    imgs_mat.append(rgb2gray(img_mat))\n",
    "    # downsize your image in case corner_detect runs slow in test\n",
    "    # imgs_mat.append(rgb2gray(img_mat)[::2, ::2])\n",
    "    crns_mat.append(corner_detect(imgs_mat[i], nCorners, smoothSTD, windowSize))\n",
    "    \n",
    "    img_war = imread('p4/warrior/warrior' + str(i) + '.png')\n",
    "    imgs_war.append(rgb2gray(img_war))\n",
    "    # imgs_war.append(rgb2gray(img_war)[::2, ::2])\n",
    "    crns_war.append(corner_detect(imgs_war[i], nCorners, smoothSTD, windowSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# match corners\n",
    "R = 15\n",
    "SSDth = 100\n",
    "matching_mat = naive_matching(imgs_mat[0]/255, imgs_mat[1]/255, crns_mat[0], crns_mat[1], R, SSDth)\n",
    "matching_war = naive_matching(imgs_war[0]/255, imgs_war[1]/255, crns_war[0], crns_war[1], R, SSDth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot matching result\n",
    "def show_matching_result(img1, img2, matching):\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(np.hstack((img1, img2)), cmap='gray') # two dino images are of different sizes, resize one before use\n",
    "    for p1, p2 in matching:\n",
    "        plt.scatter(p1[0], p1[1], s=35, edgecolors='r', facecolors='none')\n",
    "        plt.scatter(p2[0] + img1.shape[1], p2[1], s=35, edgecolors='r', facecolors='none')\n",
    "        plt.plot([p1[0], p2[0] + img1.shape[1]], [p1[1], p2[1]])\n",
    "    plt.savefig('dino_matching.png')\n",
    "    plt.show()\n",
    "    \n",
    "show_matching_result(imgs_mat[0], imgs_mat[1], matching_mat)\n",
    "show_matching_result(imgs_war[0], imgs_war[1], matching_war)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Epipolar Geometry [3 pts]\n",
    "\n",
    "Using the fundamental_matrix function, and the corresponding points provided in cor1.npy and cor2.npy, calculate the fundamental matrix. \n",
    "\n",
    "Using this fundamental matrix, plot the epipolar lines in both image pairs across all images. For this part you may want to complete the function plot_epipolar_lines. Shown your result for matrix and warrior as the figure below. <img src = \"fig/dinoEpi1.png\", alt=\"dino epipolar\" width=\"400\"/> <img src = \"fig/dinoEpi2.png\", alt=\"dino epipolar\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.misc import imread\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "\n",
    "def compute_fundamental(x1,x2):\n",
    "    \"\"\"    Computes the fundamental matrix from corresponding points \n",
    "        (x1,x2 3*n arrays) using the 8 point algorithm.\n",
    "        Each row in the A matrix below is constructed as\n",
    "        [x'*x, x'*y, x', y'*x, y'*y, y', x, y, 1] \n",
    "    \"\"\"\n",
    "    \n",
    "    n = x1.shape[1]\n",
    "    if x2.shape[1] != n:\n",
    "        raise ValueError(\"Number of points don't match.\")\n",
    "    \n",
    "    # build matrix for equations\n",
    "    A = np.zeros((n,9))\n",
    "    for i in range(n):\n",
    "        A[i] = [x1[0,i]*x2[0,i], x1[0,i]*x2[1,i], x1[0,i]*x2[2,i],\n",
    "                x1[1,i]*x2[0,i], x1[1,i]*x2[1,i], x1[1,i]*x2[2,i],\n",
    "                x1[2,i]*x2[0,i], x1[2,i]*x2[1,i], x1[2,i]*x2[2,i] ]\n",
    "            \n",
    "    # compute linear least square solution\n",
    "    U,S,V = np.linalg.svd(A)\n",
    "    F = V[-1].reshape(3,3)\n",
    "        \n",
    "    # constrain F\n",
    "    # make rank 2 by zeroing out last singular value\n",
    "    U,S,V = np.linalg.svd(F)\n",
    "    S[2] = 0\n",
    "    F = np.dot(U,np.dot(np.diag(S),V))\n",
    "    \n",
    "    return F/F[2,2]\n",
    "\n",
    "\n",
    "def fundamental_matrix(x1,x2):\n",
    "    n = x1.shape[1]\n",
    "    if x2.shape[1] != n:\n",
    "        raise ValueError(\"Number of points don't match.\")\n",
    "\n",
    "    # normalize image coordinates\n",
    "    x1 = x1 / x1[2]\n",
    "    mean_1 = np.mean(x1[:2],axis=1)\n",
    "    S1 = np.sqrt(2) / np.std(x1[:2])\n",
    "    T1 = np.array([[S1,0,-S1*mean_1[0]],[0,S1,-S1*mean_1[1]],[0,0,1]])\n",
    "    x1 = np.dot(T1,x1)\n",
    "    \n",
    "    x2 = x2 / x2[2]\n",
    "    mean_2 = np.mean(x2[:2],axis=1)\n",
    "    S2 = np.sqrt(2) / np.std(x2[:2])\n",
    "    T2 = np.array([[S2,0,-S2*mean_2[0]],[0,S2,-S2*mean_2[1]],[0,0,1]])\n",
    "    x2 = np.dot(T2,x2)\n",
    "\n",
    "    # compute F with the normalized coordinates\n",
    "    F = compute_fundamental(x1,x2)\n",
    "\n",
    "    # reverse normalization\n",
    "    F = np.dot(T1.T,np.dot(F,T2))\n",
    "\n",
    "    return F/F[2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_epipolar_lines(img1,img2, cor1, cor2):\n",
    "    \"\"\"Plot epipolar lines on image given image, corners\n",
    "\n",
    "    Args:\n",
    "        img1: Image 1.\n",
    "        img2: Image 2.\n",
    "        cor1: Corners in homogeneous image coordinate in image 1 (3xn)\n",
    "        cor2: Corners in homogeneous image coordinate in image 2 (3xn)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Your code here:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace images and corners with those of matrix and warrior\n",
    "I1 = imread(\"./p4/dino/dino0.png\")\n",
    "I2 = imread(\"./p4/dino/dino1.png\")\n",
    "\n",
    "cor1 = np.load(\"./p4/dino/cor1.npy\")\n",
    "cor2 = np.load(\"./p4/dino/cor2.npy\")\n",
    "\n",
    "plot_epipolar_lines(I1,I2,cor1,cor2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching Using Epipolar Geometry [4 pts]\n",
    "\n",
    "We will now use the epipolar geometry to build a better matching algorithm. First, detect 10 corners\n",
    "in Image1. Then, for each corner, do a linesearch along the corresponding epipolar line in Image2.\n",
    "Evaluate the SSD score for each point along this line and return the best match (or no match if all\n",
    "scores are below the SSDth). R is the radius (size) of the SSD patch in the code below.  You do not\n",
    "have to run this in both directions. Show your result as in the naive matching part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_correspondence(img1, img2, corrs):\n",
    "    \"\"\"Plot matching result on image pair given images and correspondences\n",
    "\n",
    "    Args:\n",
    "        img1: Image 1.\n",
    "        img2: Image 2.\n",
    "        corrs: Corner correspondence\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Your code here.\n",
    "    You may refer to the show_matching_result function\n",
    "    \"\"\"\n",
    "\n",
    "def correspondence_matching_epipole(img1, img2, corners1, F, R, SSDth):\n",
    "    \"\"\"Find corner correspondence along epipolar line.\n",
    "\n",
    "    Args:\n",
    "        img1: Image 1.\n",
    "        img2: Image 2.\n",
    "        corners1: Detected corners in image 1.\n",
    "        F: Fundamental matrix calculated using given ground truth corner correspondences.\n",
    "        R: SSD matching window radius.\n",
    "        SSDth: SSD matching threshold.\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "        Matching result to be used in display_correspondence function\n",
    "\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Your code here.\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace images and corners with those of matrix and warrior\n",
    "I1 = rgb2gray(imread(\"./p4/dino/dino0.png\"))\n",
    "I2 = rgb2gray(imread(\"./p4/dino/dino1.png\"))\n",
    "\n",
    "cor1 = np.load(\"./p4/dino/cor1.npy\")\n",
    "cor2 = np.load(\"./p4/dino/cor2.npy\")\n",
    "\n",
    "F = fundamental_matrix(cor1, cor2)\n",
    "\n",
    "nCorners = 10\n",
    "# detect corners using corner detector here, store in corners1\n",
    "corners1 = corner_detect(I1, nCorners, smoothSTD, windowSize)\n",
    "corrs = correspondence_matching_epipole(I1, I2, corners1, F, R, SSDth)\n",
    "display_correspondence(img1, img2, corrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triangulation [4 pts]\n",
    "Now that you have found correspondences between the pairs of images we can triangulate the corresponding 3D points. Since we do not enforce the ordering constraint the correspondences you have found are likely to be noisy and to contain a fair amount of outliers. Using the provided camera matrices you will triangulate a 3D point for each corresponding pair of points. Then by reprojecting the 3D points you will be able to find most of the outliers. You should implement the linear triangulation method described in lecture (For additional reference, see section 12.2 of ”Multiple View Geometry” by Hartley and Zisserman. The book is available electronically from any UCSD IP address.). P1 and P2 below are the camera matrices. Also write a function, find_outliers, that reprojects the world points to Image2, and then determines which points are outliers and inliers respectively. For this purpose, we will call a point an outlier if the distance between the true location, and the reprojected point location is more than 20 pixels.\n",
    "\n",
    "Display your results by showing, for image 2, the original points, the inliers, and the outliers in different markers. Compare this outlierplot with your epipolar line matching result. Does the detected outliers correspond to false matches?  \n",
    "<img src = \"fig/dinoTri.png\", alt=\"dino triangulation\" width=\"400\"/>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def triangulate(corsSSD, P1, P2):\n",
    "    \"\"\"Find corner correspondence along epipolar line.\n",
    "\n",
    "    Args:\n",
    "        corsSSD: Corner correspondence\n",
    "        P1: Projection matrix 1\n",
    "        P2: Projection matrix 2\n",
    "    \n",
    "    Returns:\n",
    "        3D points from triangulation\n",
    "        \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Your code here\n",
    "    \"\"\"\n",
    "\n",
    "def find_outliers(points3D, P2, outlierTH, corsSSD):\n",
    "    \"\"\"Find and plot outliers on image 2\n",
    "\n",
    "    Args:\n",
    "        points3D: 3D point from triangulation\n",
    "        P2: Projection matrix in for image 2\n",
    "        outlierTH: outlier pixel distance threshold\n",
    "        corsSSD: Corner correspondence\n",
    "        \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Your code here\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace images and corners with those of matrix and warrior\n",
    "I1 = rgb2gray(imread(\"./p4/dino/dino0.png\"))\n",
    "I2 = rgb2gray(imread(\"./p4/dino/dino1.png\"))\n",
    "\n",
    "cor1 = np.load(\"./p4/dino/cor1.npy\")\n",
    "cor2 = np.load(\"./p4/dino/cor2.npy\")\n",
    "\n",
    "P1 = np.load(\"./p4/dino/proj1.npy\")\n",
    "P2 = np.load(\"./p4/dino/proj2.npy\")\n",
    "\n",
    "outlierTH = 20\n",
    "F = fundamental_matrix(cor1, cor2)\n",
    "ncorners = 50\n",
    "corners1 = corner_detect(I1, ncorners, smoothSTD, windowSize)\n",
    "corsSSD = correspondence_matching_epipole(I1, I2, corners1, F, R, SSDth)\n",
    "points3D = triangulate(corsSSD, P1, P2)\n",
    "inlier, outlier = find_outliers(points3D, P2, outlierTH, corsSSD)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "123px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
